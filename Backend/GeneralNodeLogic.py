"""
General Node Logic Module - Enhanced for Weev Platform

This module contains the base class and common functionality for all node types in the system.
Designed for the "Figma for AI Agents" vision with zero-setup testing and intelligent workflows.
"""

import os
import uuid
from typing import Dict, List, Any, Optional, Callable, AsyncGenerator
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import json
import asyncio
import logging
from openai import OpenAI

# Enhanced data structures for holistic workflow management
@dataclass
class PreviousNodeOutput:
    node_id: str
    node_type: str
    data: Any
    timestamp: float
    connection_type: str  # 'direct' or 'conditional'
    success: bool = True
    error_message: Optional[str] = None
    execution_time_ms: Optional[float] = None

@dataclass
class WorkflowMemory:
    """Maintains context across the entire workflow"""
    workflow_id: str
    conversation_history: List[Dict[str, Any]] = field(default_factory=list)
    global_context: Dict[str, Any] = field(default_factory=dict)
    user_preferences: Dict[str, Any] = field(default_factory=dict)
    execution_path: List[str] = field(default_factory=list)
    
    def add_to_history(self, node_id: str, node_type: str, input_data: Any, output_data: Any):
        """Add node execution to conversation history"""
        self.conversation_history.append({
            'node_id': node_id,
            'node_type': node_type,
            'input': input_data,
            'output': output_data,
            'timestamp': datetime.now().isoformat()
        })
        self.execution_path.append(node_id)
    
    def get_relevant_context(self, max_history: int = 5) -> str:
        """Get recent conversation context for LLM"""
        recent_history = self.conversation_history[-max_history:]
        context_lines = []
        
        for entry in recent_history:
            context_lines.append(f"Previous {entry['node_type']}: {entry['output']}")
        
        return "\n".join(context_lines)

@dataclass
class NodeInputs:
    # Core 3 inputs as specified
    system_rules: str
    user_configuration: Dict[str, Any]
    previous_node_data: List[PreviousNodeOutput]
    
    # Enhanced holistic inputs
    workflow_memory: WorkflowMemory
    execution_context: Dict[str, Any] = field(default_factory=dict)
    streaming_callback: Optional[Callable[[str], None]] = None

@dataclass
class NodeOutput:
    node_id: str
    node_type: str
    data: Any
    timestamp: float
    metadata: Dict[str, Any]
    success: bool = True
    error_message: Optional[str] = None
    
    # Enhanced outputs for workflow intelligence
    next_suggested_nodes: List[str] = field(default_factory=list)
    confidence_score: Optional[float] = None
    tool_calls_made: List[str] = field(default_factory=list)
    memory_updates: Dict[str, Any] = field(default_factory=dict)
    debug_info: Dict[str, Any] = field(default_factory=dict)

class NodeExecutionMode(Enum):
    """Different execution modes for different use cases"""
    PROTOTYPE = "prototype"  # Fast, cost-effective for testing
    PRODUCTION = "production"  # Full featured, reliable
    DEBUG = "debug"  # Detailed logging and inspection

class GeneralNodeLogic:
    """
    Enhanced General Node Logic for Weev's AI Agent Workflow Platform
    
    Key enhancements for holistic operation:
    1. Workflow memory and context management
    2. Intelligent agent-first design
    3. Real-time streaming with debugging
    4. Multi-model support and optimization
    5. Production-ready error handling
    """
    
    def __init__(self, execution_mode: NodeExecutionMode = NodeExecutionMode.PROTOTYPE):
        self.execution_mode = execution_mode
        self.logger = self._setup_logging()
        
        # Initialize LLM provider with error handling
        try:
            api_key = os.environ.get('NVIDIA_API_KEY')
            if api_key:
                self.llm_provider = OpenAI(
                    api_key=api_key,
                    base_url='https://integrate.api.nvidia.com/v1',
                )
            else:
                # No local provider; expect LLMManager to be injected at runtime
                self.llm_provider = None
                self.logger.info("No local LLM provider configured; using LLMManager if available.")
        except Exception as e:
            # Do not crash here; LLMManager path may be used
            self.logger.warning(f"LLM provider init skipped: {str(e)}")
            self.llm_provider = None
        
        # Model selection based on execution mode
        self.model_config = self._get_model_config()
    
    def _setup_logging(self) -> logging.Logger:
        """Setup logging based on execution mode"""
        logger = logging.getLogger(f"weev_node_{self.execution_mode.value}")
        
        if self.execution_mode == NodeExecutionMode.DEBUG:
            logger.setLevel(logging.DEBUG)
        else:
            logger.setLevel(logging.INFO)
            
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _get_model_config(self) -> Dict[str, Any]:
        """Get model configuration based on execution mode"""
        configs = {
            NodeExecutionMode.PROTOTYPE: {
                'model': 'meta/llama-3.1-8b-instruct',
                'temperature': 0.7,
                'max_tokens': 2048,
                'top_p': 0.9
            },
            NodeExecutionMode.PRODUCTION: {
                'model': 'meta/llama-3.1-8b-instruct',
                'temperature': 0.6,
                'max_tokens': 4096,
                'top_p': 0.9
            },
            NodeExecutionMode.DEBUG: {
                'model': 'meta/llama-3.1-8b-instruct',
                'temperature': 0.5,
                'max_tokens': 1024,
                'top_p': 0.8
            }
        }
        return configs[self.execution_mode]
    
    async def execute_node(self, inputs: NodeInputs) -> NodeOutput:
        """
        Enhanced execution method with holistic workflow management
        """
        start_time = datetime.now()
        node_id = self._generate_node_id()
        
        try:
            self.logger.info(f"Starting node execution: {node_id}")
            
            # Step 1: Build intelligent context-aware prompt
            complete_prompt = self._build_intelligent_prompt(inputs)
            
            if self.execution_mode == NodeExecutionMode.DEBUG:
                self.logger.debug(f"Complete prompt: {complete_prompt[:200]}...")
            
            # Step 2: Execute with enhanced streaming and error handling
            result = await self._execute_with_intelligent_streaming(
                complete_prompt, 
                inputs.user_configuration,
                inputs.streaming_callback
            )
            
            # Step 3: Process result with workflow intelligence
            node_output = await self._format_intelligent_output(
                result, inputs, node_id, start_time
            )
            
            # Step 4: Update workflow memory
            inputs.workflow_memory.add_to_history(
                node_id, 
                node_output.node_type,
                inputs.user_configuration,
                result
            )
            
            self.logger.info(f"Node execution completed: {node_id}")
            return node_output
            
        except Exception as e:
            self.logger.error(f"Node execution failed: {str(e)}")
            return self._create_error_output(node_id, str(e), start_time)
    
    def _build_intelligent_prompt(self, inputs: NodeInputs) -> str:
        """
        Build an intelligent, context-aware prompt that leverages workflow memory
        """
        # Get relevant conversation context
        conversation_context = inputs.workflow_memory.get_relevant_context()
        
        prompt = f"""
SYSTEM RULES (Your Core Instructions):
{inputs.system_rules}

WORKFLOW CONTEXT & MEMORY:
Workflow ID: {inputs.workflow_memory.workflow_id}
Execution Path: {' -> '.join(inputs.workflow_memory.execution_path[-5:])}
Recent Conversation:
{conversation_context}

Global Context: {json.dumps(inputs.workflow_memory.global_context, indent=2)}

USER CONFIGURATION:
{self._format_user_configuration(inputs.user_configuration)}

IMMEDIATE PREVIOUS NODE DATA:
{self._format_previous_node_data(inputs.previous_node_data)}

IMPORTANT: You are part of an intelligent AI agent workflow. Consider the entire conversation context, not just immediate inputs. Make decisions that advance the workflow toward the user's ultimate goal.

Execute your node function now:
"""
        return prompt.strip()
    
    async def _execute_with_intelligent_streaming(
        self, 
        prompt: str, 
        user_config: Dict[str, Any],
        streaming_callback: Optional[Callable[[str], None]] = None
    ) -> str:
        """
        Execute with intelligent streaming, error recovery, and real-time feedback
        """
        try:
            # Merge user config with model defaults
            model_params = {**self.model_config, **user_config}

            # Prefer centralized LLMManager if available
            llm_manager = getattr(self, "llm_manager", None)
            db_session = getattr(self, "db_session", None)

            if llm_manager and db_session:
                # Prepare messages for chat API
                messages = [{"role": "user", "content": prompt}]

                # Coerce IDs to UUID if possible
                exec_id = getattr(self, "execution_id", None)
                user_id = getattr(self, "user_id", None)
                try:
                    if isinstance(exec_id, str):
                        exec_id = uuid.UUID(exec_id)
                except Exception:
                    exec_id = None
                try:
                    if isinstance(user_id, str):
                        user_id = uuid.UUID(user_id)
                except Exception:
                    user_id = None

                # Call manager (providers currently return full result; simulate streaming)
                result = await llm_manager.generate(
                    db_session,
                    user_id=user_id,
                    model=model_params['model'],
                    messages=messages,
                    stream=True,
                    temperature=model_params.get('temperature'),
                    max_tokens=model_params.get('max_tokens'),
                    extra=user_config,
                    execution_id=exec_id,
                )

                content = result.content or ""
                if not content:
                    return ""

                full_response = ""
                # Simulated streaming in chunks
                chunk_size = 200
                for i in range(0, len(content), chunk_size):
                    piece = content[i:i+chunk_size]
                    full_response += piece
                    if streaming_callback:
                        try:
                            streaming_callback(piece)
                        except Exception:
                            pass
                    else:
                        await self._stream_to_frontend(piece)
                self.logger.info(f"LLMManager response streamed (simulated): {len(full_response)} chars")
                return full_response

            # Fallback to local provider streaming if manager not available
            if self.llm_provider:
                completion = self.llm_provider.chat.completions.create(
                    model=model_params['model'],
                    messages=[{"role": "user", "content": prompt}],
                    temperature=model_params.get('temperature', 0.6),
                    top_p=model_params.get('top_p', 0.9),
                    max_tokens=model_params.get('max_tokens', 4096),
                    stream=True
                )

                full_response = ""
                chunk_count = 0
                for chunk in completion:
                    if chunk.choices[0].delta.content:
                        piece = chunk.choices[0].delta.content
                        full_response += piece
                        chunk_count += 1
                        if streaming_callback:
                            streaming_callback(piece)
                        else:
                            await self._stream_to_frontend(piece)
                        if self.execution_mode == NodeExecutionMode.DEBUG and chunk_count % 10 == 0:
                            self.logger.debug(f"Streamed {chunk_count} chunks, {len(full_response)} chars")
                self.logger.info(f"Streaming completed: {chunk_count} chunks, {len(full_response)} characters")
                return full_response

            # If no provider is available at all
            return "Error: No LLM provider available (LLMManager or local provider)."

        except Exception as e:
            self.logger.error(f"Error in LLM execution: {str(e)}")
            if "rate_limit" in str(e).lower():
                await asyncio.sleep(1)
                return await self._execute_with_intelligent_streaming(prompt, user_config, streaming_callback)
            return f"Error: {str(e)}"
    
    async def _format_intelligent_output(
        self, 
        response: str, 
        inputs: NodeInputs, 
        node_id: str,
        start_time: datetime
    ) -> NodeOutput:
        """
        Format output with intelligent analysis and workflow suggestions
        """
        execution_time = (datetime.now() - start_time).total_seconds() * 1000
        
        # Analyze response for workflow intelligence
        next_nodes = self._analyze_next_nodes(response, inputs)
        confidence = self._calculate_confidence(response, inputs)
        memory_updates = self._extract_memory_updates(response, inputs)
        
        return NodeOutput(
            node_id=node_id,
            node_type="general",  # Will be overridden by specific nodes
            data=response,
            timestamp=datetime.now().timestamp(),
            next_suggested_nodes=next_nodes,
            confidence_score=confidence,
            memory_updates=memory_updates,
            metadata={
                'execution_mode': self.execution_mode.value,
                'execution_time_ms': execution_time,
                'model_used': self.model_config['model'],
                'system_rules_hash': hash(inputs.system_rules),
                'workflow_id': inputs.workflow_memory.workflow_id,
                'previous_nodes_processed': len(inputs.previous_node_data),
                'conversation_length': len(inputs.workflow_memory.conversation_history)
            },
            debug_info={
                'prompt_length': len(self._build_intelligent_prompt(inputs)),
                'response_length': len(response),
                'streaming_chunks': response.count(' ') + 1,  # Approximate
                'memory_context_used': bool(inputs.workflow_memory.conversation_history)
            } if self.execution_mode == NodeExecutionMode.DEBUG else {}
        )
    
    def _analyze_next_nodes(self, response: str, inputs: NodeInputs) -> List[str]:
        """Analyze response to suggest next nodes in workflow"""
        suggestions = []
        
        # Simple keyword-based analysis (can be enhanced with ML)
        if any(word in response.lower() for word in ['search', 'find', 'lookup', 'information']):
            suggestions.append('KnowledgeBaseNode')
        
        if any(word in response.lower() for word in ['decide', 'choose', 'analyze', 'think']):
            suggestions.append('BrainNode')
            
        if any(word in response.lower() for word in ['format', 'output', 'result', 'final']):
            suggestions.append('OutputNode')
            
        return suggestions
    
    def _calculate_confidence(self, response: str, inputs: NodeInputs) -> float:
        """Calculate confidence score based on response quality"""
        base_confidence = 0.7
        
        # Increase confidence for longer, detailed responses
        if len(response) > 200:
            base_confidence += 0.1
            
        # Increase confidence if workflow context was used
        if inputs.workflow_memory.conversation_history:
            base_confidence += 0.1
            
        # Decrease confidence for error responses
        if 'error' in response.lower() or 'sorry' in response.lower():
            base_confidence -= 0.3
            
        return min(1.0, max(0.0, base_confidence))
    
    def _extract_memory_updates(self, response: str, inputs: NodeInputs) -> Dict[str, Any]:
        """Extract information that should be stored in workflow memory"""
        updates = {}
        
        # Extract key entities or facts mentioned in response
        # This is a simple implementation - can be enhanced with NLP
        if 'user wants' in response.lower():
            updates['user_intent'] = response
        
        if any(word in response.lower() for word in ['remember', 'important', 'key']):
            updates['important_info'] = response[:200]
            
        return updates
    
    def _create_error_output(self, node_id: str, error_message: str, start_time: datetime) -> NodeOutput:
        """Create standardized error output"""
        execution_time = (datetime.now() - start_time).total_seconds() * 1000
        
        return NodeOutput(
            node_id=node_id,
            node_type="error",
            data=f"Node execution failed: {error_message}",
            timestamp=datetime.now().timestamp(),
            success=False,
            error_message=error_message,
            confidence_score=0.0,
            metadata={
                'execution_mode': self.execution_mode.value,
                'execution_time_ms': execution_time,
                'error': True,
                'error_message': error_message
            }
        )
    
    # Keep existing helper methods with minor enhancements
    def _format_user_configuration(self, config: Dict[str, Any]) -> str:
        """Format user configuration into readable text"""
        if not config:
            return "No user configuration provided"
            
        formatted_lines = []
        for key, value in config.items():
            if key not in ['temperature', 'max_tokens', 'top_p']:  # Skip internal params
                formatted_lines.append(f"{key}: {json.dumps(value)}")
        
        return "\n".join(formatted_lines) if formatted_lines else "No user-specific configuration"
    
    def _format_previous_node_data(self, previous_data: List[PreviousNodeOutput]) -> str:
        """Format previous node outputs into readable context"""
        if not previous_data:
            return "No immediate previous node data"
        
        formatted_nodes = []
        for node in previous_data:
            status = "âœ“" if node.success else "âœ—"
            node_info = f"""
{status} From {node.node_type} ({node.node_id}):
{json.dumps(node.data, indent=2)}
{f"Error: {node.error_message}" if node.error_message else ""}
---"""
            formatted_nodes.append(node_info)
        
        return "\n".join(formatted_nodes)
    
    async def _stream_to_frontend(self, content: str) -> None:
        """Stream content to frontend - enhanced for production"""
        if self.execution_mode == NodeExecutionMode.DEBUG:
            print(f"[STREAM] {content}", end='', flush=True)
        else:
            print(content, end='', flush=True)
    
    def _generate_node_id(self) -> str:
        """Generate a UUID-based node ID for better uniqueness"""
        return f"node_{str(uuid.uuid4())[:8]}"
